{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNNs\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, height=218, width=178, dim_output=2):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.H=height\n",
    "        self.W=width\n",
    "        self.output=dim_output\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout= nn.Dropout(p=0.5) \n",
    "        self.bn1= nn.BatchNorm2d(32) \n",
    "        self.bn2= nn.BatchNorm2d(64)\n",
    "        self.bn3= nn.BatchNorm2d(128)\n",
    "        \n",
    "        #convolutions\n",
    "        self.conv1 = nn.Conv2d( 3,  32,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,  64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        #fully connected\n",
    "        self.fc1 = nn.Linear(128*(self.H//8)*(self.W//8),128) \n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,self.output)\n",
    "        \n",
    "        #sequential\n",
    "        self.layer1=nn.Sequential(self.conv1, self.bn1, self.max_pool, self.activation)\n",
    "        self.layer2=nn.Sequential(self.conv2, self.bn2, self.max_pool, self.activation)\n",
    "        self.layer3=nn.Sequential(self.conv3, self.bn3, self.max_pool, self.activation)\n",
    "        self.layer4=nn.Sequential(self.fc1, self.activation, self.dropout)\n",
    "        self.layer5=nn.Sequential(self.fc2, self.activation, self.dropout)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        \n",
    "        out=self.layer3(self.layer2(self.layer1(img)))\n",
    "        \n",
    "        ###flattening\n",
    "        batch_size =img.shape[0]\n",
    "        out=out.view(batch_size, -1)\n",
    "        \n",
    "        out=self.fc3(self.layer5(self.layer4(out)))\n",
    "       \n",
    "        return out\n",
    "    \n",
    "model=Model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load best model\n",
    "best_path=torch.load('./models/facemask.pth',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(best_path['final_model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) c:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.hpp:253: error: (-215:Assertion failed) VScn::contains(scn) && VDcn::contains(dcn) && VDepth::contains(depth) in function 'cv::CvtHelper<struct cv::Set<3,4,-1>,struct cv::Set<1,-1,-1>,struct cv::Set<0,2,5>,2>::CvtHelper'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-551857647622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mfaces_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.2) c:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.hpp:253: error: (-215:Assertion failed) VScn::contains(scn) && VDcn::contains(dcn) && VDepth::contains(depth) in function 'cv::CvtHelper<struct cv::Set<3,4,-1>,struct cv::Set<1,-1,-1>,struct cv::Set<0,2,5>,2>::CvtHelper'\n"
     ]
    }
   ],
   "source": [
    "haarcasc_path = os.path.dirname(cv2.__file__)\n",
    "face_cascade = cv2.CascadeClassifier(haarcasc_path+'/data/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(haarcasc_path+'/data/haarcascade_eye.xml')\n",
    "\n",
    "threshold = 80\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "org = (30, 30)\n",
    "mask_font_color = (0, 255, 0)  #green\n",
    "nomask_font_color = (0, 0, 255) #red\n",
    "noface_font_color=(250, 206, 135) #light blue\n",
    "thickness = 2\n",
    "font_scale = 1\n",
    "mask = \"Thank you for wearing a mask\"\n",
    "#nomask = \"Please wear a mask\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    (a , img) = cap.read()\n",
    "    img = cv2.flip(img,1)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    (thresh, bw) = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "    faces_gray = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    faces_bw = face_cascade.detectMultiScale(bw, 1.1, 4)\n",
    "    eyes = eye_cascade.detectMultiScale(gray)\n",
    "    \n",
    "    if(len(faces_gray) == 0 and len(faces_bw) == 0 and len(eyes) == 0):\n",
    "        cv2.putText(img, \"No face found\", org, font, font_scale, noface_font_color, thickness, cv2.LINE_AA)\n",
    "    else:\n",
    "        if(len(faces_gray) == 0 and len(faces_bw) == 0 and len(eyes) != 0):\n",
    "            cv2.putText(img, mask, org, font, font_scale, mask_font_color, thickness, cv2.LINE_AA)\n",
    "        else:\n",
    "            for (x, y, w, h) in faces_gray:\n",
    "                \n",
    "               \n",
    "                face_img = img[y:y+h, x:x+w]\n",
    "                rerect_sized=cv2.resize(face_img,(218,178))\n",
    "                normalized=rerect_sized/255.0\n",
    "                reshaped=np.reshape(normalized,(3,218,178))\n",
    "                reshaped = np.vstack([reshaped])\n",
    "                to_tensor=torchvision.transforms.ToTensor()\n",
    "                t_reshaped=to_tensor(reshaped)\n",
    "                tensor_reshaped=t_reshaped.unsqueeze(1).permute(1,2,3,0)\n",
    "                result=model(tensor_reshaped.float())\n",
    "                label1=torch.argmax(result, dim=1)[0]\n",
    "                label=label1.detach().item()\n",
    "                \n",
    "                if label == 1:\n",
    "                    cv2.putText(img, mask, org, font, font_scale, mask_font_color, thickness, cv2.LINE_AA)\n",
    "                    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                else:\n",
    "                    cv2.putText(img, \"no mask\", (x, y-10), font, font_scale, nomask_font_color, thickness, cv2.LINE_AA)\n",
    "                    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow('LIVE',   img)\n",
    "    key = cv2.waitKey(10)\n",
    "    \n",
    "    if key == 27: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
